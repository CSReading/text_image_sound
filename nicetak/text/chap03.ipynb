{
 "cells": [
  {
   "cell_type": "raw",
   "id": "80b01025",
   "metadata": {},
   "source": [
    "---\n",
    "title: 第３章： 基礎技術\n",
    "author: Nicetak\n",
    "date: '2022-11-20'\n",
    "format:\n",
    "  html:\n",
    "    keep-ipynb: true\n",
    "    self-contained: true\n",
    "    code-fold: show\n",
    "execute:\n",
    "  echo: false\n",
    "  warning: false\n",
    "bibliography: text_image_sound.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9c0799",
   "metadata": {},
   "source": [
    "## 形態素解析ツールについて\n",
    "\n",
    "### MeCab\n",
    "[MeCab](http://taku910.github.io/mecab/)は長らく日本語の形態素解析ツールとして使われてきたが,\n",
    "\n",
    "1. 2013年以降更新されていない\n",
    "1. ややbuildが複雑\n",
    "1. ネット上の記事において, 使用する辞書に関する記述が古い\n",
    "\n",
    "などの問題がある. 形態素解析の質は辞書によるところも大きいので, １つ目の問題はあまり問題にはなっていない.\n",
    "２つめの問題もPythonから呼び出す限りにおいては,\n",
    "[mecab-python3](https://github.com/SamuraiT/mecab-python3)\n",
    "をpipで導入すれば使えるため, 比較的小さい問題である （コマンドラインからは利用できないが.）\n",
    "\n",
    "辞書に関しては, IPAdicは2007年に, NEologdは2020年から更新されていないので,\n",
    "[mecab-python3](https://github.com/SamuraiT/mecab-python3)\n",
    "で推奨されている[UniDic](https://clrd.ninjal.ac.jp/unidic/)を選ぶのがよいだろう.\n",
    "軽量版の`unidic-lite`はpipから, 完全版は\n",
    "```\n",
    "pip install unidic\n",
    "python3 -m unidic download\n",
    "```\n",
    "でダウンロードすることができる.\n",
    "\n",
    "### Sudachi\n",
    "[Sudachi](https://github.com/WorksApplications/Sudachi)はワークスアプリケーションズ\n",
    "の徳島人工知能NLP研究所が開発している形態素解析ツール.\n",
    "リクルートのMegagon Labsと国立国語研究所が開発している\n",
    "日本語NLPライブラリの[GiNZA](https://spacy.io/)の中で, 形態素解析器として用いられている.\n",
    "GiNZAは欧米諸言語で広く利用されている[spaCy](https://spacy.io/)の\n",
    "日本語対応版といったライブラリでこれから広く利用されると見込まれている.\n",
    "\n",
    "Sudachiの辞書はUniDicとNEologdをベースに独自の修正や語彙追加が施されている.\n",
    "詳細については, 開発者の一人の久本空海さんによる\n",
    "[解説](https://zenn.dev/sorami/articles/c9a506000fd1fbd1cf98)\n",
    "などを参考にされたい.\n",
    "\n",
    "\n",
    "## 3.1\n",
    ">3.1節にあるように, 実際に形態素解析を行ってみよう.\n",
    ">3.1節と同じ結果が出ただろうか.\n",
    ">MeCabを使った形態素解析においては, どの辞書を使うかによって解析結果が異なってくる.\n",
    ">MeCabの辞書としてどのようなものがあるか調べてみよう.\n",
    ">また辞書を変えたとき, 解析結果がどのように変わるか調べてみよう.\n",
    "\n",
    "### MeCab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a3d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "import unidic_lite\n",
    "mecab_lite = MeCab.Tagger(f'-d \"{unidic_lite.DICDIR}\"')\n",
    "#import unidic\n",
    "#mecab_full = MeCab.Tagger(f'-d \"{unidic.DICDIR}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2149606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "日本\tニッポン\tニッポン\t日本\t名詞-固有名詞-地名-国\t\t\t3\n",
      "語\tゴ\tゴ\t語\t名詞-普通名詞-一般\t\t\t1\n",
      "の\tノ\tノ\tの\t助詞-格助詞\t\t\t\n",
      "文\tブン\tブン\t文\t名詞-普通名詞-一般\t\t\t1\n",
      "の\tノ\tノ\tの\t助詞-格助詞\t\t\t\n",
      "形態\tケータイ\tケイタイ\t形態\t名詞-普通名詞-一般\t\t\t0\n",
      "素\tソ\tソ\t素\t接尾辞-名詞的-一般\t\t\t\n",
      "解析\tカイセキ\tカイセキ\t解析\t名詞-普通名詞-サ変可能\t\t\t0\n",
      "を\tオ\tヲ\tを\t助詞-格助詞\t\t\t\n",
      "行っ\tオコナッ\tオコナウ\t行う\t動詞-一般\t五段-ワア行\t連用形-促音便\t0\n",
      "た\tタ\tタ\tた\t助動詞\t助動詞-タ\t終止形-一般\t\n",
      ".\t\t\t．\t補助記号-句点\t\t\t\n",
      "EOS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: false\n",
    "print(mecab_lite.parse(\"日本語の文の形態素解析を行った.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3795d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| code-fold: false\n",
    "#print(mecab_full.parse(\"日本語の文の形態素解析を行った.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3285eb3",
   "metadata": {},
   "source": [
    "### Sudachi (GiNZA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aea6110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 日本語 日本語 NOUN 名詞-普通名詞-一般 nmod 2\n",
      "1 の の ADP 助詞-格助詞 case 0\n",
      "2 文 文 NOUN 名詞-普通名詞-一般 nmod 5\n",
      "3 の の ADP 助詞-格助詞 case 2\n",
      "4 形態素 形態素 NOUN 名詞-普通名詞-一般 compound 5\n",
      "5 解析 解析 NOUN 名詞-普通名詞-サ変可能 obj 7\n",
      "6 を を ADP 助詞-格助詞 case 5\n",
      "7 行っ 行う VERB 動詞-一般 ROOT 7\n",
      "8 た た AUX 助動詞 aux 7\n",
      "9 . . PUNCT 補助記号-句点 punct 7\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "\n",
    "doc = nlp(\"日本語の文の形態素解析を行った.\")\n",
    "\n",
    "for sent in doc.sents:\n",
    "    for token in sent:\n",
    "        print(token.i, token.orth_, token.lemma_, token.pos_, \n",
    "              token.tag_, token.dep_, token.head.i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6be5819",
   "metadata": {},
   "source": [
    "現在のサポート状況やライブラリの数などを鑑みて, 以降はGiNZAを用いる.\n",
    "\n",
    "## 3.2\n",
    ">新聞記事など, 身近なテキストをダウンロードし, それを形態素解析してみよう.\n",
    ">その上で, 出現する形態素ごとにその形態素が出現する頻度を数えるプログラムを書いてみよう.\n",
    ">その結果は元のテキストの特徴を表しているだろうか？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7217bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url = \"https://nlp100.github.io/data/neko.txt\"\n",
    "df = pd.read_csv(url, header = None)\n",
    "df.rename(columns={0 : \"raw\"}, inplace=True)\n",
    "\n",
    "indices = [df.index[df.raw == s][0] for s in [\"一\", \"二\", \"三\", \"四\", \"五\", \"六\", \"七\", \"八\", \"九\", \"十\", \"十一\"]]\n",
    "indices.append(len(df))\n",
    "chapter = []\n",
    "for i in range(1, len(indices)):\n",
    "    chapter.extend([i] * (indices[i] - indices[i-1]))\n",
    "\n",
    "df[\"chapter\"] = chapter\n",
    "df.drop(index = indices[0:-1], inplace = True)\n",
    "df.reset_index(drop=True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec52558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [nlp(raw) for (raw, chap) in zip(df.raw, df.chapter) if chap == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a7925b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "d_count = defaultdict(int)\n",
    "d_count_noun = defaultdict(int)\n",
    "for doc in docs:\n",
    "    for sent in doc.sents:\n",
    "       for token in sent:\n",
    "            if token.pos_ != \"PUNC\":\n",
    "                d_count[token.lemma_] += 1\n",
    "                if token.pos_ == \"NOUN\":\n",
    "                    d_count_noun[token.lemma_] += 1\n",
    "\n",
    "d_count = sorted(d_count.items(), key=lambda x: x[1], reverse=True)\n",
    "d_count_noun = sorted(d_count_noun.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75c7db",
   "metadata": {},
   "source": [
    ":::: {.columns}\n",
    "::: {.column}\n",
    "\n",
    "**すべての形態素**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b4658d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('の', 384),\n",
       " ('。', 329),\n",
       " ('て', 308),\n",
       " ('だ', 289),\n",
       " ('は', 271),\n",
       " ('を', 252),\n",
       " ('と', 239),\n",
       " ('た', 218),\n",
       " ('に', 217),\n",
       " ('が', 207)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_count[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35171b2b",
   "metadata": {},
   "source": [
    ":::\n",
    "::: {.column}\n",
    "\n",
    "**名詞のみ**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d1419a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('吾輩', 81),\n",
       " ('もの', 53),\n",
       " ('事', 49),\n",
       " ('主人', 38),\n",
       " ('猫', 23),\n",
       " ('人', 19),\n",
       " ('人間', 18),\n",
       " ('時', 18),\n",
       " ('眼', 18),\n",
       " ('家', 18)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_count_noun[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd170e",
   "metadata": {},
   "source": [
    ":::\n",
    "::::\n",
    "\n",
    "## 3.3\n",
    ">口語的な文では係り受けに交差が生じることがある.\n",
    ">係り受けに交差が生じている文を考えてみよう.\n",
    ">またそのような文を解析するためにはどのような考え方が必要になるか, 考えてみよう.\n",
    "\n",
    "「これが私は正しいと思う.」\n",
    "\n",
    "- これ $\\rightarrow$ 正しい\n",
    "- 私 $\\rightarrow$ 思う\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96874b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "これ : これが → 正しい\n",
      "私 : 私は → 正しい\n",
      "正しい : 正しいと → 思う\n"
     ]
    }
   ],
   "source": [
    "import ginza\n",
    "text = \"これが私は正しいと思う.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for span in ginza.bunsetu_phrase_spans(doc):\n",
    "    for token in span.lefts:\n",
    "        print(f'{token} : {str(ginza.bunsetu_span(token))} → {str(span)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573546e2",
   "metadata": {},
   "source": [
    "係り受けに交差を許す場合, すべての2単語のペアについて解析する必要がある.\n",
    "そのため非効率 $(O(n^3))$ に思われていたが, @mcdonald_non-projective_2005\n",
    "はこの Non-projective dependency parsing 問題が, 最小全域木問題に帰着し, $O(n^2)$\n",
    "の計算量であることを示した.\n",
    "\n",
    "\n",
    "## 3.4\n",
    ">新聞記事などから一つ文を選び, その分の修辞構造木を作成してみよう.\n",
    ">作成したら, 友人などにも同じように修辞構造木を作成してもらい, 自分のものと比較してみよう.\n",
    ">修辞構造木は注釈付が難しいことが知られており,\n",
    ">同じ文に対しても注釈者によって別の構造が付与されることがままある.\n",
    ">その原因についても考えてみよう.\n",
    "\n",
    "Skip."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
