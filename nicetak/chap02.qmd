---
title: "第２章: 言語資源と言語モデル"
author: Nicetak
date: '2022-11-20'
format:
  html:
    code-fold: true
    self-contained: true
    keep-ipynb: true
bibliography: text_image_sound.bib
jupyter: python3
---

## 2.1
>上に述べたようにシーソラスやオントロジーにはさまざまなものがある.
>インターネットを利用してこれらのシーソラスを利用することができるので,
>それぞれのシーソラスの差異を調べてみよう.
>どのような違いが見られ, またその違いはなぜ生じるのか, 考えてみよう.

[日本の言語資源・ツールのカタログ](https://www.jaist.ac.jp/project/NLP_Portal/doc/LR/lr-cat-j.html)にまとまっている.
網羅的かつ無料であるものとして, 国立国語研究所の[分類語彙表](https://clrd.ninjal.ac.jp/goihyo.html)
とNICTの[日本語版WordNet](https://bond-lab.github.io/wnja/)がある.

分類語表が体言用言の分類から始まる木構造であるのに対し, WordNetは類義語をネットワークとして繋いでいる.

## 2.2
>Nグラム言語モデルで単語列が生起する確率を求めるとき,
>コーパス中に現れない単語が出現する問題をゼロ頻度問題と呼ぶ.
>このようなときにどのような対処法が考えられるだろうか.

**参考**

Statistical Machine Translation [@sokolov_statistical_2015]

- [Lecture 5](https://www.cl.uni-heidelberg.de/courses/ss15/smt/scribe5.pdf)
- [Lecture 6](https://www.cl.uni-heidelberg.de/courses/ss15/smt/scribe6.pdf)


### Add-$\alpha$ (Laplace) Smoothing

すべてのN-gramカウントを $\alpha$ だけ出現回数を増やすことで, 近似的な確率を求める.
$V$ を全単語の集合として,

$$
\begin{aligned}
p(w_i|w_{i-N+1}^{i-1})
&=\frac{c(w_{i-N+1}^{i})}{\sum_{w_i}c(w_{i-N+1}^{i})} \\
&\simeq\frac{\alpha + c(w_{i-N+1}^{i})}{\alpha |V| + \sum_{w_i} c(w_{i-N+1}^{i-1})}
\end{aligned}
$$


大部分の意味のない （確率ゼロの） N-gram にパイが取られてしまうため,
本来の意味ある （確率非ゼロの） N-gramの確率が過小に推定されてしまう.

その他の修正方法として, Deleted-estimate Smoothing, Good-Turing Smoothing,
Back-Off and Interpolation, Witten-Bell Smoothing
等がある. 詳細は上記のスライドを参考のこと.


## 2.3
>近年ではNグラム言語モデル以外にも様々な言語モデルが提案されている.
>どのような言語モデルが他に存在するか, 調べてみよう.

### Latent Variable Model [@arora_latent_2016]

持田先生による （日本語での） [解説](http://chasen.org/~daiti-m/paper/SNLP8-latent.pdf)
を読むことができる.

単語空間の語数を $d$ として, あるランダムウォークする文脈ベクトル $c_t \in \mathcal{R}^d$
を仮定する. また各単語は潜在ベクトル $v_w \in \mathcal{R}$ を持つと考える.
さらに, 単語 $w$ の時間$t$における生成確率を以下のように仮定する.

$$\Pr\left(w \text{ emitted at time } t \mid c_t\right) \propto \exp(\langle c_t, v_w \rangle)$$

この時, ある単語ペアの自己相互情報量 (Pointwise Mutual Information, PMI)に関して,

$$
\begin{aligned}
PMI(w, w') &:= \log \frac{p(w, w')}{p(w)p(w')} \\
&= \frac{\langle v_w, v_{w'} \rangle}{d} \pm O(\epsilon)
\end{aligned}
$$

が成り立つことが示せる.
Skip-gramベースのWord2Vecの学習はこのPMIの推定を行っていることと同値であり,
得られる word embeddings はここで仮定している, 潜在ベクトルの推定値である.
このモデルは, Word2Vec (や GloVe) の内積がPMIを近似しているという実証的な事実に理論的な説明を与えた.

また, このモデルは, 単語生成過程をモデル化しており同時確率を計算できる点で Generative model の一つである.
Skip-gram モデルは条件付き確率自体のモデル化であり, Discrimiive modelの一つであり, 対地される概念のモデルである.