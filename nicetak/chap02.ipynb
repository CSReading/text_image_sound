{
 "cells": [
  {
   "cell_type": "raw",
   "id": "711ae0c2",
   "metadata": {},
   "source": [
    "---\n",
    "title: '第２章: 言語資源と言語モデル'\n",
    "author: Nicetak\n",
    "date: '2022-11-20'\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    self-contained: true\n",
    "    keep-ipynb: true\n",
    "bibliography: text_image_sound.bib\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6273b6",
   "metadata": {},
   "source": [
    "## 2.1\n",
    ">上に述べたようにシーソラスやオントロジーにはさまざまなものがある.\n",
    ">インターネットを利用してこれらのシーソラスを利用することができるので,\n",
    ">それぞれのシーソラスの差異を調べてみよう.\n",
    ">どのような違いが見られ, またその違いはなぜ生じるのか, 考えてみよう.\n",
    "\n",
    "[日本の言語資源・ツールのカタログ](https://www.jaist.ac.jp/project/NLP_Portal/doc/LR/lr-cat-j.html)にまとまっている.\n",
    "網羅的かつ無料であるものとして, 国立国語研究所の[分類語彙表](https://clrd.ninjal.ac.jp/goihyo.html)\n",
    "とNICTの[日本語版WordNet](https://bond-lab.github.io/wnja/)がある.\n",
    "\n",
    "分類語表が体言用言の分類から始まる木構造であるのに対し, WordNetは類義語をネットワークとして繋いでいる.\n",
    "\n",
    "## 2.2\n",
    ">Nグラム言語モデルで単語列が生起する確率を求めるとき,\n",
    ">コーパス中に現れない単語が出現する問題をゼロ頻度問題と呼ぶ.\n",
    ">このようなときにどのような対処法が考えられるだろうか.\n",
    "\n",
    "**参考**\n",
    "\n",
    "Statistical Machine Translation [@sokolov_statistical_2015]\n",
    "\n",
    "- [Lecture 5](https://www.cl.uni-heidelberg.de/courses/ss15/smt/scribe5.pdf)\n",
    "- [Lecture 6](https://www.cl.uni-heidelberg.de/courses/ss15/smt/scribe6.pdf)\n",
    "\n",
    "\n",
    "### Add-$\\alpha$ (Laplace) Smoothing\n",
    "\n",
    "すべてのN-gramカウントを $\\alpha$ だけ出現回数を増やすことで, 近似的な確率を求める.\n",
    "$V$ を全単語の集合として,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(w_i|w_{i-N+1}^{i-1})\n",
    "&=\\frac{c(w_{i-N+1}^{i})}{\\sum_{w_i}c(w_{i-N+1}^{i})} \\\\\n",
    "&\\simeq\\frac{\\alpha + c(w_{i-N+1}^{i})}{\\alpha |V| + \\sum_{w_i} c(w_{i-N+1}^{i-1})}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "大部分の意味のない （確率ゼロの） N-gram にパイが取られてしまうため,\n",
    "本来の意味ある （確率非ゼロの） N-gramの確率が過小に推定されてしまう.\n",
    "\n",
    "その他の修正方法として, Deleted-estimate Smoothing, Good-Turing Smoothing,\n",
    "Back-Off and Interpolation, Witten-Bell Smoothing\n",
    "等がある. 詳細は上記のスライドを参考のこと.\n",
    "\n",
    "\n",
    "## 2.3\n",
    ">近年ではNグラム言語モデル以外にも様々な言語モデルが提案されている.\n",
    ">どのような言語モデルが他に存在するか, 調べてみよう.\n",
    "\n",
    "### Latent Variable Model [@arora_latent_2016]\n",
    "\n",
    "持田先生による （日本語での） [解説](http://chasen.org/~daiti-m/paper/SNLP8-latent.pdf)\n",
    "を読むことができる.\n",
    "\n",
    "単語空間の語数を $d$ として, あるランダムウォークする文脈ベクトル $c_t \\in \\mathcal{R}^d$\n",
    "を仮定する. また各単語は潜在ベクトル $v_w \\in \\mathcal{R}$ を持つと考える.\n",
    "さらに, 単語 $w$ の時間$t$における生成確率を以下のように仮定する.\n",
    "\n",
    "$$\\Pr\\left(w \\text{ emitted at time } t \\mid c_t\\right) \\propto \\exp(\\langle c_t, v_w \\rangle)$$\n",
    "\n",
    "この時, ある単語ペアの自己相互情報量 (Pointwise Mutual Information, PMI)に関して,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "PMI(w, w') &:= \\log \\frac{p(w, w')}{p(w)p(w')} \\\\\n",
    "&= \\frac{\\langle v_w, v_{w'} \\rangle}{d} \\pm O(\\epsilon)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "が成り立つことが示せる.\n",
    "Skip-gramベースのWord2Vecの学習はこのPMIの推定を行っていることと同値であり,\n",
    "得られる word embeddings はここで仮定している, 潜在ベクトルの推定値である.\n",
    "このモデルは, Word2Vec (や GloVe) の内積がPMIを近似しているという実証的な事実に理論的な説明を与えた.\n",
    "\n",
    "また, このモデルは, 単語生成過程をモデル化しており同時確率を計算できる点で Generative model の一つである.\n",
    "Skip-gram モデルは条件付き確率自体のモデル化であり, Discrimiive modelの一つであり, 対地される概念のモデルである."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
